{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nilmtk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This module should be used when you have no access to the SQL database of Pecan\n",
    "Street, e.g. for university access.\n",
    "To use this script, you have to first extract the CSV files from their TAR\n",
    "archives and to download the `metadata.csv` file.\n",
    "NB: If you have direct access to the SQL database, please prefer using the other\n",
    "converter named `download_dataport.py`.\n",
    "\"\"\"\n",
    "\n",
    "import nilmtk.datastore\n",
    "import nilmtk.measurement\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import re\n",
    "import shutil\n",
    "import tempfile\n",
    "import yaml\n",
    "\n",
    "from collections import OrderedDict\n",
    "from nilm_metadata import convert_yaml_to_hdf5\n",
    "from nilmtk.utils import get_module_directory\n",
    "\n",
    "\n",
    "DATA_AVAILABILITIES = [ \"97%\", \"98%\", \"99%\", \"100%\" ]\n",
    "METADATA_COLS = [\n",
    "        \"dataid\", \"building_type\", \"city\", \"state\", \"house_construction_year\",\n",
    "]\n",
    "DATABASE_TZ = \"US/Central\"\n",
    "METER_COLS = [ (\"power\", \"active\") ]\n",
    "COL_MAPPING = {\n",
    "        \"air1\": {\"type\": \"air conditioner\"},\n",
    "        \"air2\": {\"type\": \"air conditioner\"},\n",
    "        \"air3\": {\"type\": \"air conditioner\"},\n",
    "        \"airwindowunit1\": {\"type\": \"air conditioner\"},\n",
    "        \"aquarium1\": {\"type\": \"appliance\"},\n",
    "        \"bathroom1\": {\"type\": \"sockets\", \"room\": \"bathroom\"},\n",
    "        \"bathroom2\": {\"type\": \"sockets\", \"room\": \"bathroom\"},\n",
    "        \"bedroom1\": {\"type\": \"sockets\", \"room\": \"bedroom\"},\n",
    "        \"bedroom2\": {\"type\": \"sockets\", \"room\": \"bedroom\"},\n",
    "        \"bedroom3\": {\"type\": \"sockets\", \"room\": \"bedroom\"},\n",
    "        \"bedroom4\": {\"type\": \"sockets\", \"room\": \"bedroom\"},\n",
    "        \"bedroom5\": {\"type\": \"sockets\", \"room\": \"bedroom\"},\n",
    "        \"car1\": {\"type\": \"electric vehicle\"},\n",
    "        \"car2\": {\"type\": \"electric vehicle\"},\n",
    "        \"clotheswasher1\": {\"type\": \"washing machine\"},\n",
    "        \"clotheswasher_dryg1\": {\"type\": \"washer dryer\"},\n",
    "        \"diningroom1\": {\"type\": \"sockets\", \"room\": \"dining room\"},\n",
    "        \"diningroom2\": {\"type\": \"sockets\", \"room\": \"dining room\"},\n",
    "        \"dishwasher1\": {\"type\": \"dish washer\"},\n",
    "        \"disposal1\": {\"type\": \"waste disposal unit\"},\n",
    "        \"drye1\": {\"type\": \"spin dryer\"},\n",
    "        \"dryg1\": {\"type\": \"spin dryer\"},\n",
    "        \"freezer1\": {\"type\": \"freezer\"},\n",
    "        \"furnace1\": {\"type\": \"electric furnace\"},\n",
    "        \"furnace2\": {\"type\": \"electric furnace\"},\n",
    "        \"garage1\": {\"type\": \"sockets\", \"room\": \"garage\"},\n",
    "        \"garage2\": {\"type\": \"sockets\", \"room\": \"garage\"},\n",
    "        \"grid\": {},\n",
    "        \"heater1\": {\"type\": \"electric space heater\"},\n",
    "        \"heater2\": {\"type\": \"electric space heater\"},\n",
    "        \"heater3\": {\"type\": \"electric space heater\"},\n",
    "        \"housefan1\": {\"type\": \"fan\"},\n",
    "        \"jacuzzi1\": {\"type\": \"electric hot tub heater\"},\n",
    "        \"kitchen1\": {\"type\": \"sockets\", \"room\": \"kitchen\"},\n",
    "        \"kitchen2\": {\"type\": \"sockets\", \"room\": \"kitchen\"},\n",
    "        \"kitchenapp1\": {\"type\": \"sockets\", \"room\": \"kitchen\"},\n",
    "        \"kitchenapp2\": {\"type\": \"sockets\", \"room\": \"kitchen\"},\n",
    "        \"lights_plugs1\": {\"type\": \"light\"},\n",
    "        \"lights_plugs2\": {\"type\": \"light\"},\n",
    "        \"lights_plugs3\": {\"type\": \"light\"},\n",
    "        \"lights_plugs4\": {\"type\": \"light\"},\n",
    "        \"lights_plugs5\": {\"type\": \"light\"},\n",
    "        \"lights_plugs6\": {\"type\": \"light\"},\n",
    "        \"livingroom1\": {\"type\": \"sockets\", \"room\": \"living room\"},\n",
    "        \"livingroom2\": {\"type\": \"sockets\", \"room\": \"living room\"},\n",
    "        \"microwave1\": {\"type\": \"microwave\"},\n",
    "        \"office1\": {\"type\": \"sockets\", \"room\": \"office\"},\n",
    "        \"outsidelights_plugs1\": {\"type\": \"sockets\", \"room\": \"outside\"},\n",
    "        \"outsidelights_plugs2\": {\"type\": \"sockets\", \"room\": \"outside\"},\n",
    "        \"oven1\": {\"type\": \"oven\"},\n",
    "        \"oven2\": {\"type\": \"oven\"},\n",
    "        \"pool1\": {\"type\": \"electric swimming pool heater\"},\n",
    "        \"pool2\": {\"type\": \"electric swimming pool heater\"},\n",
    "        \"poollight1\": {\"type\": \"light\"},\n",
    "        \"poolpump1\": {\"type\": \"swimming pool pump\"},\n",
    "        \"pump1\": {\"type\": \"water pump\"},\n",
    "        \"range1\": {\"type\": \"stove\"},\n",
    "        \"refrigerator1\": {\"type\": \"fridge\"},\n",
    "        \"refrigerator2\": {\"type\": \"fridge\"},\n",
    "        \"security1\": {\"type\": \"security alarm\"},\n",
    "        \"sewerpump1\": {\"type\": \"water pump\"},\n",
    "        \"shed1\": {\"type\": \"sockets\", \"room\": \"shed\"},\n",
    "        \"solar\": {},\n",
    "        \"solar2\": {},\n",
    "        \"sprinkler1\": {\"type\": \"garden sprinkler\"},\n",
    "        \"sumppump1\": {\"type\": \"water pump\", \"room\": \"basement\"},\n",
    "        \"utilityroom1\": {\"type\": \"sockets\", \"room\": \"utility room\"},\n",
    "        \"venthood1\": {\"type\": \"stove\"},\n",
    "        \"waterheater1\": {\"type\": \"electric water heating appliance\"},\n",
    "        \"waterheater2\": {\"type\": \"electric water heating appliance\"},\n",
    "        \"winecooler1\": {\"type\": \"cold appliance\"},\n",
    "        \"wellpump1\": {\"type\": \"water pump\"},\n",
    "}\n",
    "# Unused variable, all those fields need a mapping in nilm_metadata.\n",
    "NEED_MAPPING = [ \"battery1\", \"circpump1\", \"icemaker1\", \"solar\", \"solar2\", ]\n",
    "\n",
    "\n",
    "def load_dataport_metadata(csv_metadata_path):\n",
    "    \"\"\"\n",
    "    Read Dataport static metadata.csv and parse building metadata.\n",
    "    This function filters buildings with:\n",
    "        1. sufficient data availability (see DATA_AVAILABILITIES variable),\n",
    "        2. site metering,\n",
    "        3. in the given state(s).\n",
    "    Parameters\n",
    "    ----------\n",
    "    csv_metadata_path: str\n",
    "        Path to the metadata.csv file.\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Matrix of the buildings and their characteristics,\n",
    "        see METADATA_COLS variable.\n",
    "    \"\"\"\n",
    "    metadata = pd.read_csv(csv_metadata_path,\n",
    "                           engine=\"c\", encoding=\"ISO-8859-1\",\n",
    "                           skiprows=[1]) # Skip header description\n",
    "    buildings = metadata[\n",
    "            metadata.egauge_1s_data_availability.isin(DATA_AVAILABILITIES) &\n",
    "            metadata.grid.eq(\"yes\")\n",
    "    ].sort_values(by=\"dataid\")\n",
    "    buildings.reset_index(drop=True, inplace=True)\n",
    "    return buildings[[*METADATA_COLS]]\n",
    "\n",
    "\n",
    "def create_tmp_metadata_dir():\n",
    "    \"\"\"\n",
    "    Create an OS-aware temporary metadata directory.\n",
    "    dataset.yaml and meter_devices.yaml are static metadata contained by NILMTK.\n",
    "    building<x>.yaml are dynamic, however and must be procedurally generated.\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Path to the temporary metadata directory.\n",
    "    \"\"\"\n",
    "    nilmtk_static_metadata = os.path.join(\n",
    "            get_module_directory(), 'dataset_converters', 'dataport', 'metadata')\n",
    "    tmp_dir = tempfile.mkdtemp()\n",
    "    metadata_dir = os.path.join(tmp_dir, \"metadata\")\n",
    "    shutil.copytree(nilmtk_static_metadata, metadata_dir)\n",
    "    print(\"Using temporary dir for metadata:\", metadata_dir)\n",
    "    # Clear dynamic metadata (if any)\n",
    "    for f in os.listdir(metadata_dir):\n",
    "        if re.search('^building', f):\n",
    "            os.remove(join(metadata_dir, f))\n",
    "\n",
    "    return metadata_dir\n",
    "\n",
    "\n",
    "def preprocess_chunk(chunk_data, b_metadata):\n",
    "    \"\"\"\n",
    "    Clean a chunk of data coming from a Dataport CSV.\n",
    "    Drop NaN columns and all columns that are not in the COL_MAPPING variable.\n",
    "    Compute the electrical consumption based on grid and photovoltaic readings.\n",
    "    Dataport readings are in kW. They are converted in W.\n",
    "    Parameters\n",
    "    ----------\n",
    "    chunk_data: pandas.DataFrame\n",
    "        Matrix of readings with one appliance per column and the \"grid\" column\n",
    "        for the total consumption and production of the building.\n",
    "    b_metadata: dict\n",
    "        Building metdata, compatible with nilmtk_metadata.\n",
    "        This variable is modified by the present function.\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Matrix of the reading in Watts with a new column \"use\" replacing the\n",
    "        \"grid\" and \"solar\" ones.\n",
    "    \"\"\"\n",
    "    cleaned = chunk_data.dropna(axis=1, how=\"all\")\n",
    "    cols_to_drop = [ c for c in cleaned.columns if c not in COL_MAPPING ]\n",
    "    cleaned = cleaned.drop(axis=1, labels=cols_to_drop)\n",
    "    if \"solar\" in cleaned.columns or \"solar2\" in cleaned.columns:\n",
    "        gen = pd.Series(data=np.zeros(cleaned[\"grid\"].shape), index=cleaned.index)\n",
    "        b_metadata[\"energy_improvements\"] = [ \"photovoltaics\" ]\n",
    "\n",
    "        if \"solar\" in cleaned.columns:\n",
    "            gen += cleaned[\"solar\"]\n",
    "            cleaned.drop(\"solar\", axis=1, inplace=True)\n",
    "\n",
    "        if \"solar2\" in cleaned.columns:\n",
    "            gen += cleaned[\"solar2\"]\n",
    "            cleaned.drop(\"solar2\", axis=1, inplace=True)\n",
    "\n",
    "        use = cleaned[\"grid\"] + gen\n",
    "        use.name = \"use\"\n",
    "        cleaned = cleaned.join(use)\n",
    "        cleaned.drop(\"grid\", axis=1, inplace=True)\n",
    "    else:\n",
    "        cleaned.rename(columns={ \"grid\": \"use\" }, inplace=True)\n",
    "\n",
    "    return cleaned * 1000\n",
    "\n",
    "\n",
    "def create_nilmtk_metadata(building_id, dataport_metadata):\n",
    "    \"\"\"\n",
    "    Create metadata for a single building based on the nilm_metadata format.\n",
    "    Parameters\n",
    "    ----------\n",
    "    building_id: int\n",
    "        Dataport identifier for the building (\"dataid\" field).\n",
    "    dataport_metadata: pandas.DataFrame\n",
    "        Output of `load_dataport_metadata`. Handy, isn't it?\n",
    "    \"\"\"\n",
    "    locality = \", \".join(\n",
    "            dataport_metadata.loc[dataport_metadata.dataid==building_id, \"city\"].tolist() \\\n",
    "            + dataport_metadata.loc[dataport_metadata.dataid==building_id, \"state\"].tolist()\n",
    "    )\n",
    "    building_metadata = {\n",
    "            \"original_name\": int(building_id),\n",
    "            \"elec_meters\": {},\n",
    "            \"appliances\": [],\n",
    "            \"geo_location\": {\n",
    "                \"locality\": locality,\n",
    "                \"country\": \"US\",\n",
    "            },\n",
    "    }\n",
    "    return building_metadata\n",
    "\n",
    "\n",
    "def extract_building_data(csv_filename, b_id, b_metadata, chunksize, csv_b_cache):\n",
    "    \"\"\"\n",
    "    Extract and clean the data from a single CSV file for a single building.\n",
    "    Parameters\n",
    "    ----------\n",
    "    csv_filename: str\n",
    "        Path to a single Dataport CSV data file.\n",
    "    b_id: int\n",
    "        Dataport building identifier (\"dataid\" field).\n",
    "    b_metadata: dict\n",
    "        Building metdata, compatible with nilmtk_metadata.\n",
    "        This variable is modified by the present function.\n",
    "    chunksize: int\n",
    "        Number of CSV rows to load in memory for processing.\n",
    "    csv_b_cache: dict of set\n",
    "        Cache of the unique buildings per CSV file.\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A matrix of measurement indexed with increasing timestamp and with each\n",
    "        meter as a column.\n",
    "    \"\"\"\n",
    "    csv_data_gen = pd.read_csv(\n",
    "            csv_filename,\n",
    "            engine=\"c\", chunksize=chunksize,\n",
    "            index_col=[ \"local_15min\" ]\n",
    "    )\n",
    "    b_isnew = True\n",
    "    b_data = pd.DataFrame()\n",
    "    csv_isnew = csv_filename not in csv_b_cache\n",
    "    if csv_isnew:\n",
    "        csv_b_cache[csv_filename] = set()\n",
    "\n",
    "    for i, csv_chunk in enumerate(csv_data_gen):\n",
    "        # Update cache if needed.\n",
    "        b_list = set(csv_chunk[\"dataid\"].values)\n",
    "        if csv_isnew:\n",
    "            csv_b_cache[csv_filename].update(b_list)\n",
    "\n",
    "        msg = \"\\tChunk {}, extracting...\".format(i)\n",
    "        if b_id in b_list:\n",
    "            if b_isnew:\n",
    "                print(\"Building {} found in {}!\".format(b_id, csv_filename))\n",
    "                b_isnew = False\n",
    "\n",
    "            print(msg, end='\\r')\n",
    "            b_chunk = csv_chunk.loc[csv_chunk.dataid.eq(b_id)].copy()\n",
    "            b_chunk.index = pd.to_datetime(b_chunk.index, utc=True, infer_datetime_format=True)\n",
    "            b_chunk = b_chunk.tz_convert(DATABASE_TZ)\n",
    "            b_chunk = preprocess_chunk(b_chunk, b_metadata)\n",
    "            if b_data.empty:\n",
    "                b_data = b_chunk\n",
    "            else:\n",
    "                b_data = b_data.append(b_chunk)\n",
    "\n",
    "    print(\"\\t\" + \" \" * len(msg), end='\\r')\n",
    "    if not b_data.empty:\n",
    "        msg = \"\\tSorting data...\"\n",
    "        print(msg, end='\\r')\n",
    "        # mergesort is 30% faster than quicksort in this case.\n",
    "        b_data.sort_index(kind=\"mergesort\", inplace=True)\n",
    "\n",
    "    print(\"\\t\" + \" \" * len(msg), end='\\r')\n",
    "    return b_data\n",
    "\n",
    "\n",
    "def write_meter_data(hdf_store, b_id, b_data, nilmtk_metadata):\n",
    "    \"\"\"\n",
    "    Write meter data and generate meter and appliance data from a measurement\n",
    "    matrix.\n",
    "    Note:   pandas.HDFStore.put is not thread-safe.\n",
    "            (see https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#caveats)\n",
    "            Parallel implementation should include a single writer process.\n",
    "    Parameters\n",
    "    ----------\n",
    "    hdf_store: pandas.HDFStore\n",
    "        HDF file descriptor.\n",
    "        Opening and closing the descriptor is left to the caller function.\n",
    "    b_id: int\n",
    "        Dataport building identifier (\"dataid\" field).\n",
    "    b_data: pandas.DataFrame\n",
    "        Matrix of measurements indexed by timestamp with column-wise meter data.\n",
    "    nilmtk_metadata: collections.OrderedDict\n",
    "        Already initialized dictionary to store NILM metadata associated with\n",
    "        buildings, meters and appliances.\n",
    "        This variable is changed by the present function!\n",
    "    \"\"\"\n",
    "    if \"instance\" in nilmtk_metadata[b_id]:\n",
    "        nilmtk_b_id = nilmtk_metadata[b_id][\"instance\"]\n",
    "    else:\n",
    "        nilmtk_b_id = tuple(nilmtk_metadata).index(b_id) + 1\n",
    "        nilmtk_metadata[b_id][\"instance\"] = nilmtk_b_id\n",
    "\n",
    "    for m_id, meter in enumerate(b_data.columns):\n",
    "        key = nilmtk.datastore.Key(building=nilmtk_b_id, meter=m_id + 1)\n",
    "        msg = \"\\tWriting {}\".format(str(key))\n",
    "        print(msg, end='\\r')\n",
    "        # Meter data.\n",
    "        m_data = pd.DataFrame(b_data[meter])\n",
    "        m_data.columns = pd.MultiIndex.from_tuples(METER_COLS)\n",
    "        m_data.columns.set_names(nilmtk.measurement.LEVEL_NAMES, inplace=True)\n",
    "        hdf_store.put(str(key), m_data, format=\"table\", append=True)\n",
    "        hdf_store.flush()\n",
    "        # Meter & appliance metadata\n",
    "        if meter == 'use':\n",
    "            meter_metadata = { \"device_model\": \"eGauge\", \"site_meter\": True }\n",
    "        else:\n",
    "            meter_metadata = { \"device_model\": \"eGauge\", \"submeter_of\": 0 }\n",
    "            app_metadata = { \"original_name\": meter, \"meters\": [ m_id + 1, ] }\n",
    "            app_metadata.update(COL_MAPPING[meter])\n",
    "            app_id = 1\n",
    "            app_type = app_metadata[\"type\"]\n",
    "            for other in nilmtk_metadata[b_id][\"appliances\"]:\n",
    "                if other[\"type\"] == app_type:\n",
    "                    app_id += 1\n",
    "\n",
    "            app_metadata[\"instance\"] = app_id\n",
    "            nilmtk_metadata[b_id][\"appliances\"].append(app_metadata)\n",
    "\n",
    "        nilmtk_metadata[b_id][\"elec_meters\"][m_id + 1] = meter_metadata\n",
    "        print(\"\\t\" + \" \" * len(msg), end='\\r')\n",
    "\n",
    "    print(\"\\tWriting done.\")\n",
    "\n",
    "\n",
    "def convert_dataport(csv_filenames, metadata_path, hdf_filename, chunksize=1e6):\n",
    "    \"\"\"\n",
    "    Sequentially convert Dataport data and metadata to a NILMTK-compatible HDF\n",
    "    file.\n",
    "    All CSV files given as input will be converted in a single HDF file.\n",
    "    This function uses a cache to speed up the loading.\n",
    "    Expect the first pass to be slow, as the cache is not yet effective.\n",
    "    The convertion of the data from Austin and New York lasts one day.\n",
    "    Note:   It would have been more computationally efficient to load each CSV\n",
    "            file once and convert its data.\n",
    "            However, the CSV rows are not sorted by increasing timestamp.\n",
    "            Besides, the CSV are too big to fit in memory.\n",
    "            Filenames follow a chronological order, though.\n",
    "            Conclusion: we have to load the CSV files as many times as there\n",
    "            are buildings.\n",
    "    Parameters\n",
    "    ----------\n",
    "    csv_filenames: list of str\n",
    "        Array of path to the Dataport CSV data files to include in the dataset.\n",
    "    metadata_path: str\n",
    "        Path to the Dataport \"metadata.csv\" containing the building info.\n",
    "    hdf_filename: str\n",
    "        Path of the HDF dataset file that will be created.\n",
    "    chunksize: int\n",
    "        Number of CSV rows to load in memory for processing.\n",
    "        Adapt according to the available memory.\n",
    "        Defaults to 1 million rows.\n",
    "    \"\"\"\n",
    "    csv_filenames.sort()\n",
    "    csv_building_cache = {} # cache of the building list per file to speed up the search\n",
    "    store = pd.HDFStore(hdf_filename, \"w\", complevel=9, complib=\"zlib\")\n",
    "    metadata_dir = create_tmp_metadata_dir()\n",
    "    nilmtk_metadata = OrderedDict()\n",
    "    dataport_metadata = load_dataport_metadata(metadata_path)\n",
    "    dataids = [ int(i) for i in dataport_metadata[\"dataid\"].values ]\n",
    "    for b_id in dataids:\n",
    "        b_metadata = create_nilmtk_metadata(b_id, dataport_metadata)\n",
    "        for csv_file in csv_filenames:\n",
    "            print(\"Looking for building {}\".format(b_id), end='\\r')\n",
    "            if csv_file not in csv_building_cache or b_id in csv_building_cache[csv_file]:\n",
    "                b_data = extract_building_data(\n",
    "                         csv_file, b_id, b_metadata, chunksize, csv_building_cache)\n",
    "\n",
    "                if not b_data.empty:\n",
    "                    nilmtk_metadata[b_id] = b_metadata\n",
    "                    write_meter_data(store, b_id, b_data, nilmtk_metadata)\n",
    "\n",
    "    store.close()\n",
    "    for b_metadata in nilmtk_metadata.values():\n",
    "        b_name = \"building{:d}.yaml\".format(b_metadata[\"instance\"])\n",
    "        yml_filename = os.path.join(metadata_dir, b_name)\n",
    "        with open(yml_filename, \"w\") as yml_file:\n",
    "            yml_file.write(yaml.dump(b_metadata))\n",
    "\n",
    "    convert_yaml_to_hdf5(metadata_dir, hdf_filename)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    convert_dataport([\"15minute_data_newyork.csv\"], \"metadata.csv\", \"dataport.h5\", 3e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from nilmtk import DataSet, MeterGroup\n",
    "from nilmtk.utils import print_dict\n",
    "\n",
    "# plt.style.use('seaborn-dark-palette')\n",
    "\n",
    "dataport = DataSet('dataport.h5')\n",
    "print_dict(dataport.metadata)\n",
    "\n",
    "# elec = dataport.buildings[1].elec\n",
    "# print_dict(elec.buildings)\n",
    "# main = elec.mains().power_series_all_data()['2019-06-24 15:00:00-05':'2019-06-25 15:00:00-05'].resample('15min').pad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dict(dataport.buildings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dict(dataport.buildings[1].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataport.buildings[3].elec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import time\n",
    "\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from six import iteritems\n",
    "\n",
    "from nilmtk import DataSet, TimeFrame, MeterGroup, HDFDataStore\n",
    "from nilmtk.legacy.disaggregate import CombinatorialOptimisation, FHMM\n",
    "import nilmtk.utils\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = (13, 6)\n",
    "train = DataSet('dataport.h5')\n",
    "test = DataSet('dataport.h5')\n",
    "\n",
    "building = 1\n",
    "\n",
    "train.set_window(end=\"2019-07-24\")\n",
    "test.set_window(start=\"2019-07-24\")\n",
    "\n",
    "train_elec = train.buildings[1].elec\n",
    "test_elec = test.buildings[1].elec\n",
    "\n",
    "train_elec.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_train_elec = train_elec.submeters().select_top_k(k=5)\n",
    "top_5_train_elec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_elec.mains().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataport.set_window(start='2019-06-24',end='2019-06-25')\n",
    "main = dataport.buildings[1].elec.mains()\n",
    "main.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataport = DataSet('dataport.h5')\n",
    "elec = dataport.buildings[1].elec\n",
    "print(elec.mains().power_series_all_data().head(97))\n",
    "main = elec.mains().power_series_all_data()['2019-06-24 00:00:00-05:00':'2019-06-24 23:45:00-05:00'].resample('15T').pad()\n",
    "print(main)\n",
    "plt.figure()\n",
    "plt.plot(main)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access a specific data series, e.g., mains power\n",
    "mains = dataport.buildings[1].elec.mains()\n",
    "\n",
    "# Check the start time and end time of the data series\n",
    "mains_timeframe = mains.get_timeframe()\n",
    "mains_start_time = mains_timeframe.start\n",
    "mains_end_time = mains_timeframe.end\n",
    "print(\"Mains Start Time:\", mains_start_time)\n",
    "print(\"Mains End Time:\", mains_end_time)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Specify the start and end dates\n",
    "start_date = mains_start_time.date()\n",
    "end_date = mains_end_time.date()\n",
    "\n",
    "# Generate the date range\n",
    "date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "\n",
    "# Print all the dates\n",
    "for date in date_range:\n",
    "    print(date.date())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "mains = dataport.buildings[1].elec.mains()\n",
    "\n",
    "# Check the start time and end time of the data series\n",
    "mains_timeframe = mains.get_timeframe()\n",
    "mains_start_time = mains_timeframe.start\n",
    "mains_end_time = mains_timeframe.end\n",
    "\n",
    "# Specify the start and end dates\n",
    "start_date = mains_start_time.date()\n",
    "end_date = mains_end_time.date()\n",
    "\n",
    "# Generate the date range\n",
    "date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "\n",
    "# Create a list to store the main series for each date\n",
    "main_list = []\n",
    "\n",
    "# Iterate over the dates and create a new main series for each date\n",
    "for date in date_range:\n",
    "    # Construct the date string in the desired format\n",
    "    date_str = date.strftime('%Y-%m-%d')\n",
    "\n",
    "    # Create the new main series for the current date\n",
    "    new_main = elec.mains().power_series_all_data()[f'{date_str} 00:00:00-05:00':f'{date_str} 23:45:00-05:00'].resample('15T').pad()\n",
    "\n",
    "    # Append the new main series to the list\n",
    "    main_list.append(new_main)\n",
    "\n",
    "# Print the list of main series\n",
    "# for i, main in enumerate(main_list):\n",
    "#     print(f\"main{i+1}:\")\n",
    "# #     print(main)\n",
    "\n",
    "print(main_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for building_id in range(1, 2):\n",
    "    mains = dataport.buildings[building_id].elec.mains()\n",
    "    mains_timeframe = mains.get_timeframe()\n",
    "    mains_start_time = mains_timeframe.start\n",
    "    mains_end_time = mains_timeframe.end\n",
    "    # Specify the start and end dates\n",
    "    start_date = mains_start_time.date()\n",
    "    end_date = mains_end_time.date()\n",
    "    # Generate the date range\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    # Create a list to store the main series for each date\n",
    "    main_list = []\n",
    "    # Iterate over the dates and create a new main series for each date\n",
    "    for date in date_range:\n",
    "        # Construct the date string in the desired format\n",
    "        date_str = date.strftime('%Y-%m-%d')\n",
    "        # Create the new main series for the current date\n",
    "        new_main = elec.mains().power_series_all_data()[f'{date_str} 00:00:00-05:00':f'{date_str} 23:45:00-05:00'].resample('15T').pad()\n",
    "        # Append the new main series to the list\n",
    "        main_list.append(new_main)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Print the list of main series\n",
    "# for i, main in enumerate(main_list):\n",
    "#     print(f\"main{i+1}:\")\n",
    "# #     print(main)\n",
    "\n",
    "print(main_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a list to store the main series and building IDs\n",
    "# main_list = []\n",
    "# building_ids = []\n",
    "\n",
    "# # Iterate over the buildings\n",
    "# for building_id in range(1, 16):\n",
    "#     # Get the elec object for the current building\n",
    "#     elec = dataport.buildings[building_id].elec\n",
    "    \n",
    "#     # Get the main series for the current building\n",
    "#     mains = elec.mains()\n",
    "    \n",
    "#     # Check the start time and end time of the data series\n",
    "#     mains_timeframe = mains.get_timeframe()\n",
    "#     mains_start_time = mains_timeframe.start\n",
    "#     mains_end_time = mains_timeframe.end\n",
    "    \n",
    "#     # Specify the start and end dates\n",
    "#     start_date = mains_start_time.date()\n",
    "#     end_date = mains_end_time.date()\n",
    "    \n",
    "#     # Generate the date range\n",
    "#     date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    \n",
    "#     # Iterate over the dates and create a new main series for each date\n",
    "#     for date in date_range:\n",
    "#         # Construct the date string in the desired format\n",
    "#         date_str = date.strftime('%Y-%m-%d')\n",
    "        \n",
    "#         # Create the new main series for the current date and building\n",
    "#         new_main = mains.power_series_all_data()[f'{date_str} 00:00:00-05:00':f'{date_str} 23:45:00-05:00'].resample('15T').pad()\n",
    "        \n",
    "#         # Append the new main series and building ID to the lists\n",
    "#         main_list.append(new_main)\n",
    "#         building_ids.append(building_id)\n",
    "        \n",
    "#         # Print the building ID\n",
    "#         print(\"Building ID:\", building_id)\n",
    "#         print(main_list)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main = elec.mains().power_series_all_data().resample('1D').pad()\n",
    "# print(main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main1 = main_list[0]\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pyts.image import GramianAngularField\n",
    "\n",
    "gadf = GramianAngularField(image_size=96,method='difference')  # Assuming 96 corresponds to the length of a 1-day time series with 15-minute intervals\n",
    "gadf_image = gadf.fit_transform(main1.values.reshape((1,-1)))\n",
    "\n",
    "# Plot the GADF image\n",
    "plt.imshow(gadf_image[0], cmap='rainbow', origin='lower')\n",
    "plt.title('GADF Plot')\n",
    "plt.colorbar(label='GADF')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Time')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pyts.image import GramianAngularField\n",
    "\n",
    "# # Assuming 'main' is a Pandas Series containing the time series data\n",
    "# main_array = main.to_numpy()  # Convert Pandas Series to NumPy array\n",
    "\n",
    "# # Reshape the NumPy array\n",
    "# reshaped_data = main_array.reshape(1, -1)\n",
    "\n",
    "# Calculate the GADF image representation\n",
    "gadf = GramianAngularField(image_size=96,method='difference')  # Assuming 96 corresponds to the length of a 1-day time series with 15-minute intervals\n",
    "gadf_image = gadf.fit_transform(main.values.reshape((1,-1)))\n",
    "\n",
    "# Plot the GADF image\n",
    "plt.imshow(gadf_image[0], cmap='rainbow', origin='lower')\n",
    "plt.title('GADF Plot')\n",
    "plt.colorbar(label='GADF')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Time')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "gasf =GramianAngularField(image_size=96,method='summation')  # Assuming 96 corresponds to the length of a 1-day time series with 15-minute intervals\n",
    "gasf_image = gasf.fit_transform(main.values.reshape((1,-1)))\n",
    "\n",
    "# Plot the GADF image\n",
    "plt.imshow(gasf_image[0], cmap='rainbow', origin='lower')\n",
    "plt.title('GASF Plot')\n",
    "plt.colorbar(label='GASF')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Time')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert 'main' to a 2D numpy array\n",
    "main_array = main.to_numpy()\n",
    "main_2d = main_array.reshape(-1, 96)  # Assuming each day has 96 time points with 15-minute intervals\n",
    "\n",
    "# Reshape the data to include 4 pixels per hour\n",
    "reshaped_data = main_2d.reshape(-1, 4)\n",
    "\n",
    "\n",
    "# Plot the heatmap\n",
    "x_labels = np.arange(0, 24)\n",
    "plt.imshow(reshaped_data.T, cmap='coolwarm', aspect='auto', extent=[0, 23, 0, 1])\n",
    "plt.colorbar(label='Consumption Quantity')\n",
    "plt.xlabel('Time (hours)')\n",
    "plt.ylabel('Time Interval (15 minutes)')\n",
    "plt.title('Daily Load Consumption Heatmap')\n",
    "plt.xticks(x_labels)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert 'main' to a 2D numpy array\n",
    "main_array = main.to_numpy()\n",
    "main_2d = main_array.reshape(-1, 96)  # Assuming each day has 96 time points with 15-minute intervals\n",
    "\n",
    "# Reshape the data to include 4 pixels per hour\n",
    "reshaped_data = main_2d.reshape(-1, 4)\n",
    "\n",
    "# Calculate the z-score normalized data\n",
    "normalized_data = (reshaped_data - np.mean(reshaped_data)) / np.std(reshaped_data)\n",
    "\n",
    "\n",
    "# Plot the heatmap\n",
    "x_labels = np.arange(0, 24)\n",
    "y_labels = np.arange(0, 5)\n",
    "plt.imshow(normalized_data.T, cmap='coolwarm', aspect='auto', extent=[0, 23, 0, 4], origin='lower')\n",
    "plt.colorbar(label='Normalized Consumption')\n",
    "plt.xlabel('Time (hours)')\n",
    "plt.ylabel('Time Interval (15 minutes)')\n",
    "plt.title('Daily Load Consumption Heatmap')\n",
    "plt.xticks(x_labels)\n",
    "plt.yticks(y_labels)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5 = elec.submeters().select_top_k(k=5)\n",
    "top5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for building_id in range(1, 16):\n",
    "    building = dataport.buildings[building_id]\n",
    "    elec = building.elec\n",
    "    top5 = elec.submeters().select_top_k(k=10)\n",
    "    print(top5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "building = dataport.buildings[15]\n",
    "elec = building.elec\n",
    "print(elec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "building = dataport.buildings[1]\n",
    "elec = building.elec\n",
    "print(elec.appliances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_appliances = None\n",
    "\n",
    "for building_id in range(1, 7):\n",
    "    print(building_id)\n",
    "    building = dataport.buildings[building_id]\n",
    "    elec = building.elec\n",
    "\n",
    "    current_appliances = set()\n",
    "\n",
    "    for meter in elec.meters:\n",
    "        for appliance in meter.appliances:\n",
    "            current_appliances.add(appliance)\n",
    "            print(appliance)\n",
    "\n",
    "    if common_appliances is None:\n",
    "        common_appliances = current_appliances\n",
    "    else:\n",
    "        common_appliances = common_appliances & current_appliances\n",
    "\n",
    "print(\"Common Appliances:\")\n",
    "for appliance in common_appliances:\n",
    "    print(appliance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
