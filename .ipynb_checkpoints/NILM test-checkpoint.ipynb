{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nilmtk\n",
    "from nilmtk import DataSet, MeterGroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilmtk.dataset_converters.dataport import download_dataport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'nilmtk.dataset_converters.dataport.download_dataport' from '/Users/liuqianyi/opt/anaconda3/lib/python3.9/site-packages/nilmtk/dataset_converters/dataport/download_dataport.py'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_dataport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'nilmtk.dataset_converters.dataport.download_dataport' from '/Users/liuqianyi/opt/anaconda3/lib/python3.9/site-packages/nilmtk/dataset_converters/dataport/download_dataport.py'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nilmtk.dataset_converters.dataport.download_dataport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "This module should be used when you have no access to the SQL database of Pecan\n",
    "Street, e.g. for university access.\n",
    "To use this script, you have to first extract the CSV files from their TAR\n",
    "archives and to download the `metadata.csv` file.\n",
    "NB: If you have direct access to the SQL database, please prefer using the other\n",
    "converter named `download_dataport.py`.\n",
    "\"\"\"\n",
    "\n",
    "import nilmtk.datastore\n",
    "import nilmtk.measurement\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import re\n",
    "import shutil\n",
    "import tempfile\n",
    "import yaml\n",
    "\n",
    "from collections import OrderedDict\n",
    "from nilm_metadata import convert_yaml_to_hdf5\n",
    "from nilmtk.utils import get_module_directory\n",
    "\n",
    "\n",
    "DATA_AVAILABILITIES = [ \"97%\", \"98%\", \"99%\", \"100%\" ]\n",
    "METADATA_COLS = [\n",
    "        \"dataid\", \"building_type\", \"city\", \"state\", \"house_construction_year\",\n",
    "]\n",
    "DATABASE_TZ = \"US/Central\"\n",
    "METER_COLS = [ (\"power\", \"active\") ]\n",
    "COL_MAPPING = {\n",
    "        \"air1\": {\"type\": \"air conditioner\"},\n",
    "        \"air2\": {\"type\": \"air conditioner\"},\n",
    "        \"air3\": {\"type\": \"air conditioner\"},\n",
    "        \"airwindowunit1\": {\"type\": \"air conditioner\"},\n",
    "        \"aquarium1\": {\"type\": \"appliance\"},\n",
    "        \"bathroom1\": {\"type\": \"sockets\", \"room\": \"bathroom\"},\n",
    "        \"bathroom2\": {\"type\": \"sockets\", \"room\": \"bathroom\"},\n",
    "        \"bedroom1\": {\"type\": \"sockets\", \"room\": \"bedroom\"},\n",
    "        \"bedroom2\": {\"type\": \"sockets\", \"room\": \"bedroom\"},\n",
    "        \"bedroom3\": {\"type\": \"sockets\", \"room\": \"bedroom\"},\n",
    "        \"bedroom4\": {\"type\": \"sockets\", \"room\": \"bedroom\"},\n",
    "        \"bedroom5\": {\"type\": \"sockets\", \"room\": \"bedroom\"},\n",
    "        \"car1\": {\"type\": \"electric vehicle\"},\n",
    "        \"car2\": {\"type\": \"electric vehicle\"},\n",
    "        \"clotheswasher1\": {\"type\": \"washing machine\"},\n",
    "        \"clotheswasher_dryg1\": {\"type\": \"washer dryer\"},\n",
    "        \"diningroom1\": {\"type\": \"sockets\", \"room\": \"dining room\"},\n",
    "        \"diningroom2\": {\"type\": \"sockets\", \"room\": \"dining room\"},\n",
    "        \"dishwasher1\": {\"type\": \"dish washer\"},\n",
    "        \"disposal1\": {\"type\": \"waste disposal unit\"},\n",
    "        \"drye1\": {\"type\": \"spin dryer\"},\n",
    "        \"dryg1\": {\"type\": \"spin dryer\"},\n",
    "        \"freezer1\": {\"type\": \"freezer\"},\n",
    "        \"furnace1\": {\"type\": \"electric furnace\"},\n",
    "        \"furnace2\": {\"type\": \"electric furnace\"},\n",
    "        \"garage1\": {\"type\": \"sockets\", \"room\": \"garage\"},\n",
    "        \"garage2\": {\"type\": \"sockets\", \"room\": \"garage\"},\n",
    "        \"grid\": {},\n",
    "        \"heater1\": {\"type\": \"electric space heater\"},\n",
    "        \"heater2\": {\"type\": \"electric space heater\"},\n",
    "        \"heater3\": {\"type\": \"electric space heater\"},\n",
    "        \"housefan1\": {\"type\": \"fan\"},\n",
    "        \"jacuzzi1\": {\"type\": \"electric hot tub heater\"},\n",
    "        \"kitchen1\": {\"type\": \"sockets\", \"room\": \"kitchen\"},\n",
    "        \"kitchen2\": {\"type\": \"sockets\", \"room\": \"kitchen\"},\n",
    "        \"kitchenapp1\": {\"type\": \"sockets\", \"room\": \"kitchen\"},\n",
    "        \"kitchenapp2\": {\"type\": \"sockets\", \"room\": \"kitchen\"},\n",
    "        \"lights_plugs1\": {\"type\": \"light\"},\n",
    "        \"lights_plugs2\": {\"type\": \"light\"},\n",
    "        \"lights_plugs3\": {\"type\": \"light\"},\n",
    "        \"lights_plugs4\": {\"type\": \"light\"},\n",
    "        \"lights_plugs5\": {\"type\": \"light\"},\n",
    "        \"lights_plugs6\": {\"type\": \"light\"},\n",
    "        \"livingroom1\": {\"type\": \"sockets\", \"room\": \"living room\"},\n",
    "        \"livingroom2\": {\"type\": \"sockets\", \"room\": \"living room\"},\n",
    "        \"microwave1\": {\"type\": \"microwave\"},\n",
    "        \"office1\": {\"type\": \"sockets\", \"room\": \"office\"},\n",
    "        \"outsidelights_plugs1\": {\"type\": \"sockets\", \"room\": \"outside\"},\n",
    "        \"outsidelights_plugs2\": {\"type\": \"sockets\", \"room\": \"outside\"},\n",
    "        \"oven1\": {\"type\": \"oven\"},\n",
    "        \"oven2\": {\"type\": \"oven\"},\n",
    "        \"pool1\": {\"type\": \"electric swimming pool heater\"},\n",
    "        \"pool2\": {\"type\": \"electric swimming pool heater\"},\n",
    "        \"poollight1\": {\"type\": \"light\"},\n",
    "        \"poolpump1\": {\"type\": \"swimming pool pump\"},\n",
    "        \"pump1\": {\"type\": \"water pump\"},\n",
    "        \"range1\": {\"type\": \"stove\"},\n",
    "        \"refrigerator1\": {\"type\": \"fridge\"},\n",
    "        \"refrigerator2\": {\"type\": \"fridge\"},\n",
    "        \"security1\": {\"type\": \"security alarm\"},\n",
    "        \"sewerpump1\": {\"type\": \"water pump\"},\n",
    "        \"shed1\": {\"type\": \"sockets\", \"room\": \"shed\"},\n",
    "        \"solar\": {},\n",
    "        \"solar2\": {},\n",
    "        \"sprinkler1\": {\"type\": \"garden sprinkler\"},\n",
    "        \"sumppump1\": {\"type\": \"water pump\", \"room\": \"basement\"},\n",
    "        \"utilityroom1\": {\"type\": \"sockets\", \"room\": \"utility room\"},\n",
    "        \"venthood1\": {\"type\": \"stove\"},\n",
    "        \"waterheater1\": {\"type\": \"electric water heating appliance\"},\n",
    "        \"waterheater2\": {\"type\": \"electric water heating appliance\"},\n",
    "        \"winecooler1\": {\"type\": \"cold appliance\"},\n",
    "        \"wellpump1\": {\"type\": \"water pump\"},\n",
    "}\n",
    "# Unused variable, all those fields need a mapping in nilm_metadata.\n",
    "NEED_MAPPING = [ \"battery1\", \"circpump1\", \"icemaker1\", \"solar\", \"solar2\", ]\n",
    "\n",
    "\n",
    "def load_dataport_metadata(csv_metadata_path):\n",
    "    \"\"\"\n",
    "    Read Dataport static metadata.csv and parse building metadata.\n",
    "    This function filters buildings with:\n",
    "        1. sufficient data availability (see DATA_AVAILABILITIES variable),\n",
    "        2. site metering,\n",
    "        3. in the given state(s).\n",
    "    Parameters\n",
    "    ----------\n",
    "    csv_metadata_path: str\n",
    "        Path to the metadata.csv file.\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Matrix of the buildings and their characteristics,\n",
    "        see METADATA_COLS variable.\n",
    "    \"\"\"\n",
    "    metadata = pd.read_csv(csv_metadata_path,\n",
    "                           engine=\"c\", encoding=\"ISO-8859-1\",\n",
    "                           skiprows=[1]) # Skip header description\n",
    "    buildings = metadata[\n",
    "            metadata.egauge_1s_data_availability.isin(DATA_AVAILABILITIES) &\n",
    "            metadata.grid.eq(\"yes\")\n",
    "    ].sort_values(by=\"dataid\")\n",
    "    buildings.reset_index(drop=True, inplace=True)\n",
    "    return buildings[[*METADATA_COLS]]\n",
    "\n",
    "\n",
    "def create_tmp_metadata_dir():\n",
    "    \"\"\"\n",
    "    Create an OS-aware temporary metadata directory.\n",
    "    dataset.yaml and meter_devices.yaml are static metadata contained by NILMTK.\n",
    "    building<x>.yaml are dynamic, however and must be procedurally generated.\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Path to the temporary metadata directory.\n",
    "    \"\"\"\n",
    "    nilmtk_static_metadata = os.path.join(\n",
    "            get_module_directory(), 'dataset_converters', 'dataport', 'metadata')\n",
    "    tmp_dir = tempfile.mkdtemp()\n",
    "    metadata_dir = os.path.join(tmp_dir, \"metadata\")\n",
    "    shutil.copytree(nilmtk_static_metadata, metadata_dir)\n",
    "    print(\"Using temporary dir for metadata:\", metadata_dir)\n",
    "    # Clear dynamic metadata (if any)\n",
    "    for f in os.listdir(metadata_dir):\n",
    "        if re.search('^building', f):\n",
    "            os.remove(join(metadata_dir, f))\n",
    "\n",
    "    return metadata_dir\n",
    "\n",
    "\n",
    "def preprocess_chunk(chunk_data, b_metadata):\n",
    "    \"\"\"\n",
    "    Clean a chunk of data coming from a Dataport CSV.\n",
    "    Drop NaN columns and all columns that are not in the COL_MAPPING variable.\n",
    "    Compute the electrical consumption based on grid and photovoltaic readings.\n",
    "    Dataport readings are in kW. They are converted in W.\n",
    "    Parameters\n",
    "    ----------\n",
    "    chunk_data: pandas.DataFrame\n",
    "        Matrix of readings with one appliance per column and the \"grid\" column\n",
    "        for the total consumption and production of the building.\n",
    "    b_metadata: dict\n",
    "        Building metdata, compatible with nilmtk_metadata.\n",
    "        This variable is modified by the present function.\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Matrix of the reading in Watts with a new column \"use\" replacing the\n",
    "        \"grid\" and \"solar\" ones.\n",
    "    \"\"\"\n",
    "    cleaned = chunk_data.dropna(axis=1, how=\"all\")\n",
    "    cols_to_drop = [ c for c in cleaned.columns if c not in COL_MAPPING ]\n",
    "    cleaned = cleaned.drop(axis=1, labels=cols_to_drop)\n",
    "    if \"solar\" in cleaned.columns or \"solar2\" in cleaned.columns:\n",
    "        gen = pd.Series(data=np.zeros(cleaned[\"grid\"].shape), index=cleaned.index)\n",
    "        b_metadata[\"energy_improvements\"] = [ \"photovoltaics\" ]\n",
    "\n",
    "        if \"solar\" in cleaned.columns:\n",
    "            gen += cleaned[\"solar\"]\n",
    "            cleaned.drop(\"solar\", axis=1, inplace=True)\n",
    "\n",
    "        if \"solar2\" in cleaned.columns:\n",
    "            gen += cleaned[\"solar2\"]\n",
    "            cleaned.drop(\"solar2\", axis=1, inplace=True)\n",
    "\n",
    "        use = cleaned[\"grid\"] + gen\n",
    "        use.name = \"use\"\n",
    "        cleaned = cleaned.join(use)\n",
    "        cleaned.drop(\"grid\", axis=1, inplace=True)\n",
    "    else:\n",
    "        cleaned.rename(columns={ \"grid\": \"use\" }, inplace=True)\n",
    "\n",
    "    return cleaned * 1000\n",
    "\n",
    "\n",
    "def create_nilmtk_metadata(building_id, dataport_metadata):\n",
    "    \"\"\"\n",
    "    Create metadata for a single building based on the nilm_metadata format.\n",
    "    Parameters\n",
    "    ----------\n",
    "    building_id: int\n",
    "        Dataport identifier for the building (\"dataid\" field).\n",
    "    dataport_metadata: pandas.DataFrame\n",
    "        Output of `load_dataport_metadata`. Handy, isn't it?\n",
    "    \"\"\"\n",
    "    locality = \", \".join(\n",
    "            dataport_metadata.loc[dataport_metadata.dataid==building_id, \"city\"].tolist() \\\n",
    "            + dataport_metadata.loc[dataport_metadata.dataid==building_id, \"state\"].tolist()\n",
    "    )\n",
    "    building_metadata = {\n",
    "            \"original_name\": int(building_id),\n",
    "            \"elec_meters\": {},\n",
    "            \"appliances\": [],\n",
    "            \"geo_location\": {\n",
    "                \"locality\": locality,\n",
    "                \"country\": \"US\",\n",
    "            },\n",
    "    }\n",
    "    return building_metadata\n",
    "\n",
    "\n",
    "def extract_building_data(csv_filename, b_id, b_metadata, chunksize, csv_b_cache):\n",
    "    \"\"\"\n",
    "    Extract and clean the data from a single CSV file for a single building.\n",
    "    Parameters\n",
    "    ----------\n",
    "    csv_filename: str\n",
    "        Path to a single Dataport CSV data file.\n",
    "    b_id: int\n",
    "        Dataport building identifier (\"dataid\" field).\n",
    "    b_metadata: dict\n",
    "        Building metdata, compatible with nilmtk_metadata.\n",
    "        This variable is modified by the present function.\n",
    "    chunksize: int\n",
    "        Number of CSV rows to load in memory for processing.\n",
    "    csv_b_cache: dict of set\n",
    "        Cache of the unique buildings per CSV file.\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A matrix of measurement indexed with increasing timestamp and with each\n",
    "        meter as a column.\n",
    "    \"\"\"\n",
    "    csv_data_gen = pd.read_csv(\n",
    "            csv_filename,\n",
    "            engine=\"c\", chunksize=chunksize,\n",
    "            index_col=[ \"localminute\" ]\n",
    "    )\n",
    "    b_isnew = True\n",
    "    b_data = pd.DataFrame()\n",
    "    csv_isnew = csv_filename not in csv_b_cache\n",
    "    if csv_isnew:\n",
    "        csv_b_cache[csv_filename] = set()\n",
    "\n",
    "    for i, csv_chunk in enumerate(csv_data_gen):\n",
    "        # Update cache if needed.\n",
    "        b_list = set(csv_chunk[\"dataid\"].values)\n",
    "        if csv_isnew:\n",
    "            csv_b_cache[csv_filename].update(b_list)\n",
    "\n",
    "        msg = \"\\tChunk {}, extracting...\".format(i)\n",
    "        if b_id in b_list:\n",
    "            if b_isnew:\n",
    "                print(\"Building {} found in {}!\".format(b_id, csv_filename))\n",
    "                b_isnew = False\n",
    "\n",
    "            print(msg, end='\\r')\n",
    "            b_chunk = csv_chunk.loc[csv_chunk.dataid.eq(b_id)].copy()\n",
    "            b_chunk.index = pd.to_datetime(b_chunk.index, utc=True, infer_datetime_format=True)\n",
    "            b_chunk = b_chunk.tz_convert(DATABASE_TZ)\n",
    "            b_chunk = preprocess_chunk(b_chunk, b_metadata)\n",
    "            if b_data.empty:\n",
    "                b_data = b_chunk\n",
    "            else:\n",
    "                b_data = b_data.append(b_chunk)\n",
    "\n",
    "    print(\"\\t\" + \" \" * len(msg), end='\\r')\n",
    "    if not b_data.empty:\n",
    "        msg = \"\\tSorting data...\"\n",
    "        print(msg, end='\\r')\n",
    "        # mergesort is 30% faster than quicksort in this case.\n",
    "        b_data.sort_index(kind=\"mergesort\", inplace=True)\n",
    "\n",
    "    print(\"\\t\" + \" \" * len(msg), end='\\r')\n",
    "    return b_data\n",
    "\n",
    "\n",
    "def write_meter_data(hdf_store, b_id, b_data, nilmtk_metadata):\n",
    "    \"\"\"\n",
    "    Write meter data and generate meter and appliance data from a measurement\n",
    "    matrix.\n",
    "    Note:   pandas.HDFStore.put is not thread-safe.\n",
    "            (see https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#caveats)\n",
    "            Parallel implementation should include a single writer process.\n",
    "    Parameters\n",
    "    ----------\n",
    "    hdf_store: pandas.HDFStore\n",
    "        HDF file descriptor.\n",
    "        Opening and closing the descriptor is left to the caller function.\n",
    "    b_id: int\n",
    "        Dataport building identifier (\"dataid\" field).\n",
    "    b_data: pandas.DataFrame\n",
    "        Matrix of measurements indexed by timestamp with column-wise meter data.\n",
    "    nilmtk_metadata: collections.OrderedDict\n",
    "        Already initialized dictionary to store NILM metadata associated with\n",
    "        buildings, meters and appliances.\n",
    "        This variable is changed by the present function!\n",
    "    \"\"\"\n",
    "    if \"instance\" in nilmtk_metadata[b_id]:\n",
    "        nilmtk_b_id = nilmtk_metadata[b_id][\"instance\"]\n",
    "    else:\n",
    "        nilmtk_b_id = tuple(nilmtk_metadata).index(b_id) + 1\n",
    "        nilmtk_metadata[b_id][\"instance\"] = nilmtk_b_id\n",
    "\n",
    "    for m_id, meter in enumerate(b_data.columns):\n",
    "        key = nilmtk.datastore.Key(building=nilmtk_b_id, meter=m_id + 1)\n",
    "        msg = \"\\tWriting {}\".format(str(key))\n",
    "        print(msg, end='\\r')\n",
    "        # Meter data.\n",
    "        m_data = pd.DataFrame(b_data[meter])\n",
    "        m_data.columns = pd.MultiIndex.from_tuples(METER_COLS)\n",
    "        m_data.columns.set_names(nilmtk.measurement.LEVEL_NAMES, inplace=True)\n",
    "        hdf_store.put(str(key), m_data, format=\"table\", append=True)\n",
    "        hdf_store.flush()\n",
    "        # Meter & appliance metadata\n",
    "        if meter == 'use':\n",
    "            meter_metadata = { \"device_model\": \"eGauge\", \"site_meter\": True }\n",
    "        else:\n",
    "            meter_metadata = { \"device_model\": \"eGauge\", \"submeter_of\": 0 }\n",
    "            app_metadata = { \"original_name\": meter, \"meters\": [ m_id + 1, ] }\n",
    "            app_metadata.update(COL_MAPPING[meter])\n",
    "            app_id = 1\n",
    "            app_type = app_metadata[\"type\"]\n",
    "            for other in nilmtk_metadata[b_id][\"appliances\"]:\n",
    "                if other[\"type\"] == app_type:\n",
    "                    app_id += 1\n",
    "\n",
    "            app_metadata[\"instance\"] = app_id\n",
    "            nilmtk_metadata[b_id][\"appliances\"].append(app_metadata)\n",
    "\n",
    "        nilmtk_metadata[b_id][\"elec_meters\"][m_id + 1] = meter_metadata\n",
    "        print(\"\\t\" + \" \" * len(msg), end='\\r')\n",
    "\n",
    "    print(\"\\tWriting done.\")\n",
    "\n",
    "\n",
    "def convert_dataport(csv_filenames, metadata_path, hdf_filename, chunksize=1e6):\n",
    "    \"\"\"\n",
    "    Sequentially convert Dataport data and metadata to a NILMTK-compatible HDF\n",
    "    file.\n",
    "    All CSV files given as input will be converted in a single HDF file.\n",
    "    This function uses a cache to speed up the loading.\n",
    "    Expect the first pass to be slow, as the cache is not yet effective.\n",
    "    The convertion of the data from Austin and New York lasts one day.\n",
    "    Note:   It would have been more computationally efficient to load each CSV\n",
    "            file once and convert its data.\n",
    "            However, the CSV rows are not sorted by increasing timestamp.\n",
    "            Besides, the CSV are too big to fit in memory.\n",
    "            Filenames follow a chronological order, though.\n",
    "            Conclusion: we have to load the CSV files as many times as there\n",
    "            are buildings.\n",
    "    Parameters\n",
    "    ----------\n",
    "    csv_filenames: list of str\n",
    "        Array of path to the Dataport CSV data files to include in the dataset.\n",
    "    metadata_path: str\n",
    "        Path to the Dataport \"metadata.csv\" containing the building info.\n",
    "    hdf_filename: str\n",
    "        Path of the HDF dataset file that will be created.\n",
    "    chunksize: int\n",
    "        Number of CSV rows to load in memory for processing.\n",
    "        Adapt according to the available memory.\n",
    "        Defaults to 1 million rows.\n",
    "    \"\"\"\n",
    "    csv_filenames.sort()\n",
    "    csv_building_cache = {} # cache of the building list per file to speed up the search\n",
    "    store = pd.HDFStore(hdf_filename, \"w\", complevel=9, complib=\"zlib\")\n",
    "    metadata_dir = create_tmp_metadata_dir()\n",
    "    nilmtk_metadata = OrderedDict()\n",
    "    dataport_metadata = load_dataport_metadata(metadata_path)\n",
    "    dataids = [ int(i) for i in dataport_metadata[\"dataid\"].values ]\n",
    "    for b_id in dataids:\n",
    "        b_metadata = create_nilmtk_metadata(b_id, dataport_metadata)\n",
    "        for csv_file in csv_filenames:\n",
    "            print(\"Looking for building {}\".format(b_id), end='\\r')\n",
    "            if csv_file not in csv_building_cache or b_id in csv_building_cache[csv_file]:\n",
    "                b_data = extract_building_data(\n",
    "                         csv_file, b_id, b_metadata, chunksize, csv_building_cache)\n",
    "\n",
    "                if not b_data.empty:\n",
    "                    nilmtk_metadata[b_id] = b_metadata\n",
    "                    write_meter_data(store, b_id, b_data, nilmtk_metadata)\n",
    "\n",
    "    store.close()\n",
    "    for b_metadata in nilmtk_metadata.values():\n",
    "        b_name = \"building{:d}.yaml\".format(b_metadata[\"instance\"])\n",
    "        yml_filename = os.path.join(metadata_dir, b_name)\n",
    "        with open(yml_filename, \"w\") as yml_file:\n",
    "            yml_file.write(yaml.dump(b_metadata))\n",
    "\n",
    "    convert_yaml_to_hdf5(metadata_dir, hdf_filename)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    csvs = [\n",
    "            \"1s_data_austin_file1/1s_data_austin_file1.csv\",\n",
    "            \"1s_data_austin_file2/1s_data_austin_file2.csv\",\n",
    "            \"1s_data_austin_file3/1s_data_austin_file3.csv\",\n",
    "            \"1s_data_austin_file4/1s_data_austin_file4.csv\",\n",
    "            \"1s_data_newyork_file1/1s_data_newyork_file1.csv\",\n",
    "            \"1s_data_newyork_file2/1s_data_newyork_file2.csv\",\n",
    "            \"1s_data_newyork_file3/1s_data_newyork_file3.csv\",\n",
    "            \"1s_data_newyork_file4/1s_data_newyork_file4.csv\",\n",
    "    ]\n",
    "    convert_dataport(csvs, \"metadata.csv\", \"dataport.h5\", 3e6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
